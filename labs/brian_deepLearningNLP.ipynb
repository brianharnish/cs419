{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "brian_deepLearningNLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianharnish/cs419/blob/master/labs/brian_deepLearningNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHAxGRCXb7X_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omp4N9HXb-b4"
      },
      "source": [
        "\n",
        "# Deep Learning for Natural Language Processing\n",
        "\n",
        "Natural Language Processing (NLP) covers machine learning techniques dealing with text and includes\n",
        "\n",
        "* classification\n",
        "  * sentiment analyis (is this tweet a Pro-Biden or Anti-Biden one)\n",
        "  * stylometrics (was this typed suicide note really from the deceased or did the murderer write it [1](https://www.rosette.com/case-studies/alias/), [2](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3107011/)\n",
        "  * general classification (out of 40 topics, which is this article about)\n",
        "* question answering (building systems that can answer questions -- *What is the best treatment for hemangiosarcoma in dogs?*)\n",
        "* machine translation\n",
        "* speech recognition\n",
        "\n",
        "among many others. Deep Learning has led to tremendous improvements in all these areas of NLP. \n",
        "\n",
        "In this notebook, we are going to examine classification systems for textual information.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm5JKEGBvFZH"
      },
      "source": [
        "\n",
        "## Analyzing and Classifying Text\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/tiles.jpg\" width=\"500\"/>\n",
        "\n",
        "So far we have been dealing with **structured data**. Structured data is ... well ... structured. This means that an instance of our data has nice attributes that can be represented in a DataFrame or a table:\n",
        "\n",
        "make | mpg | cylinders | HP | 0-60 |\n",
        "---- | :---: | :---: | :---: | :---: |\n",
        "Fiat | 38 | 4 | 157   | 6.9 \n",
        "Ford F150 | 19 | 6 | 386 | 6.3 \n",
        "Mazda 3 | 37 | 4 | 155 |  7.5 \n",
        "Ford Escape | 27 | 4 | 245 | 7.1 \n",
        "Kia Soul | 31 | 4 | 164 | 8.5 \n",
        "\n",
        "The majority of data in the world is **unstructured**. Take text for example. Suppose I have a corpus of twitter posts from President Trump and the Dalai Lama and my goal is to create a classifier that takes a tweet and tells me if it was produced by Trump or the Dalai Lama:\n",
        "\n",
        "*The purpose of education is to build a happier society, we need a more holistic approach that promotes the practice of love and compassion.*\n",
        "\n",
        "*How low has President Obama gone to tapp my phones during the very sacred election*\n",
        "\n",
        "We might consider  the columns of a table to be things like *first word of the tweet*, *second word of the tweet* and so on:\n",
        "\n",
        "\n",
        "id | word 1 | word 2 | word 3 | word 4 |word 5 |word 6 | ... |\n",
        "---- | :---: | :---: | :---: | :---: | :---: |:---: |:---: |\n",
        "1 | The | purpose | of   | education |is | to | ...\n",
        "2 | How | low | has |President | Obama | gone | ...\n",
        "\n",
        "So we would be counting how many times the word *President* occurred as the fourth word of a tweet. **But that would be the wrong way to go**. First, the deep learning models we have developed so far require input of a specific length. For example, we resized our dog and cat images to a uniform 150x150 and because each image was a uniform size we could specify the input shape of our network to be ....\n",
        "\n",
        "```\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
        "```\n",
        "\n",
        "But what should the input length be for tweets? Sometimes there are short, one-word, tweets like *nice*. The average length of a tweet is 30 characters so something like the six word *Today, Toaster is 10 years old.*  And the limit of course is 280 characters so...\n",
        "\n",
        "> Like anything else, life has a beginning and in due course must end. In between those two events the important goal should be to live meaningfully, not to create trouble for others. If we can do that, when the end comes, we can go feeling at peace.\n",
        "\n",
        "That's 47 words so maybe we can limit our input to 50 words and for shorter tweets we can pad them with blank words.  But there is another possibility ...\n",
        "\n",
        "### Bag of Words (bow)\n",
        "\n",
        "A more common way to represent text is to treat the text as an unordered set of words, which is called the **bag of words** approach. \n",
        "\n",
        "\n",
        "<img src=\"http://zacharski.org/files/courses/cs419/BagofWords.jpg\" width=\"350\"/>\n",
        "\n",
        "With the bag of words approach we count word occurrences and the features (what we might think of as columns) are the unique words. For example, we have a collection of Trump and Dalai Lama tweets and indicate whether the word occurred in the tweet or not. So we might get something like:\n",
        "\n",
        "id | a | the | compassion | love |sad |fake | rigged | ... |\n",
        "---- | :---: | :---: | :---: | :---: | :---: |:---: |:---: | :---:\n",
        "Trump_1 | 1 | 0 | 0   | 0 |1 | 1 | 1 |...\n",
        "Trump_2 | 1 | 1 | 0   | 0 | 0 | 1 | 1 |...\n",
        "DalaiLama_1 | 1 | 1 | 1 |1 | 0 | 0 | 0 | ...\n",
        "\n",
        "So, for example, in DalaiLama_1, in the text there was\n",
        "\n",
        "* an occurrence of *a*\n",
        "* an occurrence of *the*\n",
        "* an occurrence of *compassion*\n",
        "* an occurrence of *love*\n",
        "\n",
        "We don't know what order the words occurred in, we just know what words occurred in the tweet. This is the bag-of-words method. \n",
        "\n",
        "Instead of short text snippets like tweets, let's say we are analyzing speeches of Trump and the Dalai Lama. Maybe then we will count how many times they used each word. So something like:\n",
        "\n",
        "id | a | the | compassion | love |sad |fake | rigged | ... |\n",
        "---- | :---: | :---: | :---: | :---: | :---: |:---: |:---: | :---:\n",
        "Trump_1 | 52 | 25 | 0   | 0 |21 | 82 | 19 |...\n",
        "Trump_2 | 30 | 35 | 0   | 0 | 5 | 20 | 31 |...\n",
        "DalaiLama_1 | 60 | 271 | 27 |63 | 12 | 0 | 0 | ...\n",
        "\n",
        "Of course we are still faced with how many columns to make, representing the vocabulary size. We could limit it to the 10,000 most common words, for example.\n",
        "\n",
        "Once we have the text in this format we can use the standard deep learning classification techniques we used before.\n",
        "\n",
        "\n",
        "Converting **unstructured** text to something **structured** is a multistep process. Let's learn the bits before putting it together. And we will start with the last step first-- creating the bag of words.\n",
        "\n",
        "### Import Keras ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-igfitKjuYru",
        "outputId": "d8f6aa17-c9ad-4c7a-fdd1-ff3c4b257c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1153Guw8wB_"
      },
      "source": [
        "### Some sample data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1mf095J8u_0",
        "outputId": "cad831b6-4028-428c-cce9-2b107ca163f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "trump1 = \"How low has President Obama gone to tapp my phones during the very sacred election process. This is Nixon/Watergate. Obama bad (or sick) guy! Sad\"\n",
        "trump2 = \"Our wonderful new Healthcare Bill is now out for review and negotiation. ObamaCare is a complete and total disaster - is imploding fast! Sad\"\n",
        "trump3 = \"Don't let the FAKE NEWS tell you that there is big infighting in the Trump Admin. We are getting along great, and getting major things done!\"\n",
        "trump4 = \"Russia talk is FAKE NEWS put out by the Dems, and played up by the media, in order to mask the big election defeat and the illegal leaks! Sad\"\n",
        "dalaiLama1 = \"The purpose of education is to build a happier society, we need a more holistic approach that promotes the practice of love and compassion.\"\n",
        "dalaiLama2 = \"Be a kind and compassionate person. This is the inner beauty that is a key factor to making a better world.\"\n",
        "dalaiLama3 = \"If our goal is a happier, more peaceful world in the future, only education will bring change.\"\n",
        "dalaiLama4 = \"Love and compassion are important, because they strengthen us. This is a source of hope\"\n",
        "tinyCorpus = [trump1, trump2, trump3, trump4, dalaiLama1, dalaiLama2, dalaiLama3, dalaiLama4]\n",
        "tinyCorpus"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How low has President Obama gone to tapp my phones during the very sacred election process. This is Nixon/Watergate. Obama bad (or sick) guy! Sad',\n",
              " 'Our wonderful new Healthcare Bill is now out for review and negotiation. ObamaCare is a complete and total disaster - is imploding fast! Sad',\n",
              " \"Don't let the FAKE NEWS tell you that there is big infighting in the Trump Admin. We are getting along great, and getting major things done!\",\n",
              " 'Russia talk is FAKE NEWS put out by the Dems, and played up by the media, in order to mask the big election defeat and the illegal leaks! Sad',\n",
              " 'The purpose of education is to build a happier society, we need a more holistic approach that promotes the practice of love and compassion.',\n",
              " 'Be a kind and compassionate person. This is the inner beauty that is a key factor to making a better world.',\n",
              " 'If our goal is a happier, more peaceful world in the future, only education will bring change.',\n",
              " 'Love and compassion are important, because they strengthen us. This is a source of hope']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-y3dlin89du"
      },
      "source": [
        "### Create the bag of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrWO8riu80Bm",
        "outputId": "258978a5-7b30-4ad2-872e-e017b25e81a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=200)\n",
        "tokenizer.fit_on_texts(tinyCorpus)\n",
        "\n",
        "# Directly get the one-hot binary representations.\n",
        "# Note that other vectorization modes than one-hot encoding are supported!\n",
        "one_hot_results = tokenizer.texts_to_matrix(tinyCorpus, mode='binary')\n",
        "# let's look at an example of an encoding ...\n",
        "print(one_hot_results[0])\n",
        "\n",
        "\n",
        "# This is how you can recover the word index that was computed\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Found 117 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p13Daqp__dfB"
      },
      "source": [
        "That was pretty easy. And now we have the texts in a form we can use for deep learning.\n",
        "\n",
        "Instead of the binary choice (a 1 if the word is present and a 0 if not) -- `mode='binary'` we can count how many occurrences of each word there were in the text:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgOPTdZf_sBy",
        "outputId": "41d584ce-d715-455f-cb8d-ee1d16f51ec3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "one_hot_results = tokenizer.texts_to_matrix(tinyCorpus, mode='count')\n",
        "# let's look at an example of an encoding ...\n",
        "print(one_hot_results[1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 3. 2. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skKDJALg_-Is"
      },
      "source": [
        "So in the first tweet:\n",
        "\n",
        "> 'Our wonderful new Healthcare Bill is now out for review and negotiation. ObamaCare is a complete and total disaster - is imploding fast! Sad',\n",
        "\n",
        "There were 3 occurrences of the word *is*, 2 of *and*, and so on as indicated in the first row above.\n",
        "\n",
        "How do we know what columns are associated with which words? We can use the word_index.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwaZVV8fAMnL",
        "outputId": "b1415639-8610-419e-ca0b-68877fa8e86f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# tokenizer.word_index is a python dictionary containing the word as a key and the column as its value\n",
        "[(k, v) for k, v in sorted(tokenizer.word_index.items(), key=lambda item: item[1])]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 1),\n",
              " ('is', 2),\n",
              " ('and', 3),\n",
              " ('a', 4),\n",
              " ('to', 5),\n",
              " ('this', 6),\n",
              " ('sad', 7),\n",
              " ('that', 8),\n",
              " ('in', 9),\n",
              " ('of', 10),\n",
              " ('obama', 11),\n",
              " ('election', 12),\n",
              " ('our', 13),\n",
              " ('out', 14),\n",
              " ('fake', 15),\n",
              " ('news', 16),\n",
              " ('big', 17),\n",
              " ('we', 18),\n",
              " ('are', 19),\n",
              " ('getting', 20),\n",
              " ('by', 21),\n",
              " ('education', 22),\n",
              " ('happier', 23),\n",
              " ('more', 24),\n",
              " ('love', 25),\n",
              " ('compassion', 26),\n",
              " ('world', 27),\n",
              " ('how', 28),\n",
              " ('low', 29),\n",
              " ('has', 30),\n",
              " ('president', 31),\n",
              " ('gone', 32),\n",
              " ('tapp', 33),\n",
              " ('my', 34),\n",
              " ('phones', 35),\n",
              " ('during', 36),\n",
              " ('very', 37),\n",
              " ('sacred', 38),\n",
              " ('process', 39),\n",
              " ('nixon', 40),\n",
              " ('watergate', 41),\n",
              " ('bad', 42),\n",
              " ('or', 43),\n",
              " ('sick', 44),\n",
              " ('guy', 45),\n",
              " ('wonderful', 46),\n",
              " ('new', 47),\n",
              " ('healthcare', 48),\n",
              " ('bill', 49),\n",
              " ('now', 50),\n",
              " ('for', 51),\n",
              " ('review', 52),\n",
              " ('negotiation', 53),\n",
              " ('obamacare', 54),\n",
              " ('complete', 55),\n",
              " ('total', 56),\n",
              " ('disaster', 57),\n",
              " ('imploding', 58),\n",
              " ('fast', 59),\n",
              " (\"don't\", 60),\n",
              " ('let', 61),\n",
              " ('tell', 62),\n",
              " ('you', 63),\n",
              " ('there', 64),\n",
              " ('infighting', 65),\n",
              " ('trump', 66),\n",
              " ('admin', 67),\n",
              " ('along', 68),\n",
              " ('great', 69),\n",
              " ('major', 70),\n",
              " ('things', 71),\n",
              " ('done', 72),\n",
              " ('russia', 73),\n",
              " ('talk', 74),\n",
              " ('put', 75),\n",
              " ('dems', 76),\n",
              " ('played', 77),\n",
              " ('up', 78),\n",
              " ('media', 79),\n",
              " ('order', 80),\n",
              " ('mask', 81),\n",
              " ('defeat', 82),\n",
              " ('illegal', 83),\n",
              " ('leaks', 84),\n",
              " ('purpose', 85),\n",
              " ('build', 86),\n",
              " ('society', 87),\n",
              " ('need', 88),\n",
              " ('holistic', 89),\n",
              " ('approach', 90),\n",
              " ('promotes', 91),\n",
              " ('practice', 92),\n",
              " ('be', 93),\n",
              " ('kind', 94),\n",
              " ('compassionate', 95),\n",
              " ('person', 96),\n",
              " ('inner', 97),\n",
              " ('beauty', 98),\n",
              " ('key', 99),\n",
              " ('factor', 100),\n",
              " ('making', 101),\n",
              " ('better', 102),\n",
              " ('if', 103),\n",
              " ('goal', 104),\n",
              " ('peaceful', 105),\n",
              " ('future', 106),\n",
              " ('only', 107),\n",
              " ('will', 108),\n",
              " ('bring', 109),\n",
              " ('change', 110),\n",
              " ('important', 111),\n",
              " ('because', 112),\n",
              " ('they', 113),\n",
              " ('strengthen', 114),\n",
              " ('us', 115),\n",
              " ('source', 116),\n",
              " ('hope', 117)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-F8umxI-JnV"
      },
      "source": [
        "So, *the, is, and, a, to* are the words represented by the first five columns.\n",
        "\n",
        "\n",
        "# TF-IDF representation\n",
        "So far we looked at \n",
        "\n",
        "* a binary bag-of-words (whether or not the word was present in the text).\n",
        "* a raw count bag-of-words (counting how many occurrences of each word)\n",
        "\n",
        "There are several other approaches\n",
        "We could represent a document as a bag of words and their probabilities (`mode=\"freq\"`). For example, in *Tom Sawyer* 4.6% of the words are *the* and 0.95% are *Tom*. But the word *the* probably occurs in most novels with that frequency. So in some sense the word *the* is uninteresting. On the other hand *Tom* probably occurs much more frequently in *Tom Sawyer* than it does in *Moby Dick* and, in that way, it is a more interesting word. One way to discount words that occur evenly throughout our document collection is to use TF-IDF.  \n",
        "\n",
        "* TF: Term Frequency - each word uprated by how often the word occurs in the document.\n",
        "* IDF Inverse Document Frequency - how often the word appears in the entire corpus\n",
        "\n",
        "and the formula is\n",
        "\n",
        "### $$ tfidf(t, d) = tf(t,d) \\times idf(t) $$\n",
        "\n",
        "where *t* is the term (the word) and *d* is the document.\n",
        "\n",
        "To explain this I will use some made up data--the word counts of 5 emails (and for the sake of later computations let's assume that each email is 100 words long):\n",
        "\n",
        "id | the | sad | compassion |  \n",
        "----: | :---: | :---: | :---:\n",
        "1 | 3 | 0 | 1 \n",
        "2 | 3 | 0 | 0 \n",
        "3 | 4 | 0 | 0 \n",
        "4 | 3 | 2 | 0 \n",
        "5 | 3 | 0 | 2\n",
        "\n",
        "\n",
        "The intuition is this. Even though the word *the* occurs frequently in each email, it is unlikely to help us classify email because it occurs in **every** email. The words *sad* and *compassion* are more interesting as they don't occur uniformly in our collection. \n",
        "\n",
        "#### TF\n",
        "\n",
        "The TF part of TF-IDF refers to how often the word occurs in the document. There are a number of ways to define TF. The simplist is to use the raw count.  So for example, the TF of *the* in document 1 is 3 (the word *the* occurred 3 times in document 1). One problem with this approach is that the raw count is influenced by the length of the document. So if you in your 1,000 word essay on Tom Sachs use 50 occurrences of *the* and I in my 90,000 word Zen van life mystery novel use 4,5000 occurrences of *the*, it doesn't mean that I am a bigger fan of *the* than you are. Even though there is that disparity in the raw counts it is not a characteristic that will help us distinguish texts about Zen from those about Tom Sachs.  In both the 1,000 word essay and the 90,000 word book about 5% of the words are *the*. A popular measure of TF is to divide the number of occurrences of a word by the total words in the document. So the TF of the word *the* in both your 1,000 word essay and my novel would be .05\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### IDF\n",
        "\n",
        "IDF is defined as:\n",
        "\n",
        "### $$ idf(t)=\\log\\frac{1+n_d}{1+df(d,t)}+ 1 $$\n",
        "\n",
        "$n_d$ is the total number of documents and $df(d,t)$ is how many documents the term *t* occurred in. \n",
        "\n",
        "So:\n",
        "\n",
        "### $$ idf(the)=\\log\\frac{1+5}{1+5}+ 1 =  1.5 $$\n",
        "\n",
        "### $$ idf(compassion)=\\log\\frac{1+5}{1+2}+ 1 = \\log{2} + 1 =  2 $$\n",
        "\n",
        "So, *the* in document 1 has a tf-idf of $.03 \\times 1.5 = 0.045$ and *compassion* has a tf-idf of $.01 \\times \\ 2 = 0.02$\n",
        "\n",
        "This is a fairly important concept to understand. I was asked about tf-idf in my oral exam to become a certified instructor at the Deep Learning Institute and fortunately I knew about it. \n",
        "\n",
        "It is also important to know that while it works well as a heuristic and has been around since the 70s, there really is no theoretical foundation to it. \n",
        "\n",
        "With all that as background, it is easy to convert a document collection into an array of TFIDF values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KVXgJ3R9Pmf",
        "outputId": "6e6efb35-6409-40b3-fd1a-7cbeb384b02a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "one_hot_results = tokenizer.texts_to_matrix(tinyCorpus, mode='tfidf')\n",
        "# let's look at an example of an encoding ...\n",
        "print(one_hot_results[1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.         1.33469384 1.29041528 0.84729786 0.\n",
            " 0.         1.09861229 0.         0.         0.         0.\n",
            " 0.         1.29928298 1.29928298 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         1.60943791 1.60943791\n",
            " 1.60943791 1.60943791 1.60943791 1.60943791 1.60943791 1.60943791\n",
            " 1.60943791 1.60943791 1.60943791 1.60943791 1.60943791 1.60943791\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTRonUNXFZRj"
      },
      "source": [
        "Again, once we have this representation we can use the deep learning methods we already used.\n",
        "\n",
        "\n",
        "## An initial example - IMDB\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/imdb.png)\n",
        "\n",
        "The Internet Movie DataBase contains 50,000 movie reviews -- 25,000 in training and 25,000 in  testing. It contains an equal number of positive and negative reviews. The task of identifying the affect of a text (whether it is postive or negative, or how strongly someone feels about the topic) is called **sentiment analysis**. \n",
        "\n",
        "### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLDR_eiY1YCL",
        "outputId": "d9d98ed8-b22b-4eca-d089-857d8492d67a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget http://zacharski.org/files/courses/cs419/imdb.zip\n",
        "!unzip imdb.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-17 15:57:24--  http://zacharski.org/files/courses/cs419/imdb.zip\n",
            "Resolving zacharski.org (zacharski.org)... 198.199.65.227\n",
            "Connecting to zacharski.org (zacharski.org)|198.199.65.227|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26559010 (25M) [application/zip]\n",
            "Saving to: ‘imdb.zip’\n",
            "\n",
            "imdb.zip            100%[===================>]  25.33M  8.84MB/s    in 2.9s    \n",
            "\n",
            "2020-11-17 15:57:27 (8.84 MB/s) - ‘imdb.zip’ saved [26559010/26559010]\n",
            "\n",
            "Archive:  imdb.zip\n",
            "  inflating: imdb.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLkosiEaRc2W",
        "outputId": "1de6a039-b81e-4f9c-f26c-aa6958f0bbee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 90608\n",
            "-rw-r--r-- 1 root root 66212309 Oct 31 18:01 imdb.csv\n",
            "-rw-r--r-- 1 root root 26559010 Oct 31 18:07 imdb.zip\n",
            "drwxr-xr-x 1 root root     4096 Nov 13 17:33 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0tpCqv_BkWF",
        "outputId": "31b65637-1111-4ecc-e7fb-2a88fd8175f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('imdb.csv')\n",
        "data"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>I thought this movie did a down right good job...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>I am a Catholic taught in parochial elementary...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>I'm going to have to disagree with the previou...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>No one expects the Star Trek movies to be high...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review sentiment\n",
              "0      One of the other reviewers has mentioned that ...  positive\n",
              "1      A wonderful little production. <br /><br />The...  positive\n",
              "2      I thought this was a wonderful way to spend ti...  positive\n",
              "3      Basically there's a family where a little boy ...  negative\n",
              "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "...                                                  ...       ...\n",
              "49995  I thought this movie did a down right good job...  positive\n",
              "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
              "49997  I am a Catholic taught in parochial elementary...  negative\n",
              "49998  I'm going to have to disagree with the previou...  negative\n",
              "49999  No one expects the Star Trek movies to be high...  negative\n",
              "\n",
              "[50000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg97QeqW2KoT"
      },
      "source": [
        "Now let's separate the texts from the labels. Also note that the labels are the strings *positive* and *negative* so let's convert those to 1 and 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql-GmhdXLShN",
        "outputId": "ded1269a-fad7-42af-ca61-1f9e0154ea39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_text = data.review\n",
        "data_label =data.sentiment\n",
        "data_label =  data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
        "data_label "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        1\n",
              "1        1\n",
              "2        1\n",
              "3        0\n",
              "4        1\n",
              "        ..\n",
              "49995    1\n",
              "49996    0\n",
              "49997    0\n",
              "49998    0\n",
              "49999    0\n",
              "Name: sentiment, Length: 50000, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXn9kHUb2cn1"
      },
      "source": [
        "### TFIDF\n",
        "Now we are going to convert the text represented as strings to a tfidf representation. \n",
        "\n",
        "Let's use the 5,000 most common words in the documents (`Tokenizer(num_words=5000)`)\n",
        "\n",
        "This will take a bit of time!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOjTzQdOJz-Z",
        "outputId": "3315fdb0-365f-46fa-88d7-dd69a7a0700b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(data_text)\n",
        "\n",
        "# Directly get the one-hot binary representations.\n",
        "# Note that other vectorization modes than one-hot encoding are supported!\n",
        "one_hot_results = tokenizer.texts_to_matrix(data_text, mode='tfidf')\n",
        "# let's look at an example of an encoding ...\n",
        "print(one_hot_results[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         2.63159248 1.98374445 ... 0.         0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dC35HHYgbUqW"
      },
      "source": [
        "###  Divide into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbPLee2_MJEd",
        "outputId": "7b82f163-05c1-4827-ef69-36699d22bd7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "imdb_train_text, imdb_test_text, imdb_train_labels, imdb_test_labels = train_test_split(one_hot_results, data_label, test_size = 0.2, random_state=42)\n",
        "imdb_test_labels\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33553    1\n",
              "9427     1\n",
              "199      0\n",
              "12447    1\n",
              "39489    0\n",
              "        ..\n",
              "28567    0\n",
              "25079    1\n",
              "18707    1\n",
              "15200    0\n",
              "5857     1\n",
              "Name: sentiment, Length: 10000, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04NGZqtVdOpF"
      },
      "source": [
        "### Build a deep learning model\n",
        "Let's go with a basic, no frills, model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj35GJladKJs",
        "outputId": "afa34835-29c3-4195-f311-5d06fb8b3bef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3BxwR6u3YnG"
      },
      "source": [
        "In our tokenizer we specified a vocabulary size of 5,000 words, so that is our `input_shape`. We are trying to predict a binary 1,0 classification so we need \n",
        "\n",
        "```\n",
        "network.add(layers.Dense(1, activation='sigmoid'))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Pb3pBO7dpS4"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(512, activation='relu', input_shape=(5000,)))\n",
        "network.add(layers.Dense(256, activation='relu'))\n",
        "network.add(layers.Dense(128, activation='relu'))\n",
        "network.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tqZ-8UO30wu"
      },
      "source": [
        "Again, we are predicting a binary 1,0 classification (was it a positive review or not) so we will use `binary_crossentropy` as our loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNchu2tmerGA"
      },
      "source": [
        "from keras import optimizers\n",
        "network.compile(optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5PiezmrexYS",
        "outputId": "47dce1e6-9c65-429a-d865-f88f55ff1886",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "network.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 512)               2560512   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,724,865\n",
            "Trainable params: 2,724,865\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTbirosf4JSO"
      },
      "source": [
        "### fitting to the data\n",
        "Now it is time to fit the network to the data. Let's use 20% of the data for validation and run for 30 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEP-UYpIe2Vb",
        "outputId": "13e3d9cd-c0f2-4124-8ade-a9b912ac79e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "history = network.fit(\n",
        "      imdb_train_text, imdb_train_labels,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=30,\n",
        "      validation_split=0.2,\n",
        "      validation_steps=50)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.4368 - accuracy: 0.8096 - val_loss: 0.3027 - val_accuracy: 0.8831\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2249 - accuracy: 0.9167 - val_loss: 0.2902 - val_accuracy: 0.8821\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1505 - accuracy: 0.9488 - val_loss: 0.2949 - val_accuracy: 0.8913\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1006 - accuracy: 0.9676 - val_loss: 0.3256 - val_accuracy: 0.8890\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0619 - accuracy: 0.9825 - val_loss: 0.3898 - val_accuracy: 0.8790\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0338 - accuracy: 0.9916 - val_loss: 0.4340 - val_accuracy: 0.8842\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0158 - accuracy: 0.9967 - val_loss: 0.5207 - val_accuracy: 0.8816\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0065 - accuracy: 0.9987 - val_loss: 0.6342 - val_accuracy: 0.8791\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.7072 - val_accuracy: 0.8802\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.7814 - val_accuracy: 0.8779\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.8748 - val_accuracy: 0.8777\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 1.3114e-04 - accuracy: 1.0000 - val_loss: 0.9793 - val_accuracy: 0.8775\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 7.8158e-05 - accuracy: 1.0000 - val_loss: 1.1066 - val_accuracy: 0.8781\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 1.2902e-05 - accuracy: 1.0000 - val_loss: 1.2557 - val_accuracy: 0.8770\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 2.6325e-06 - accuracy: 1.0000 - val_loss: 1.3893 - val_accuracy: 0.8775\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 6.4006e-07 - accuracy: 1.0000 - val_loss: 1.4844 - val_accuracy: 0.8777\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 2.5149e-07 - accuracy: 1.0000 - val_loss: 1.5509 - val_accuracy: 0.8767\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 1.2648e-07 - accuracy: 1.0000 - val_loss: 1.5983 - val_accuracy: 0.8764\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 7.3901e-08 - accuracy: 1.0000 - val_loss: 1.6317 - val_accuracy: 0.8761\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 4.8468e-08 - accuracy: 1.0000 - val_loss: 1.6597 - val_accuracy: 0.8763\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 3.4346e-08 - accuracy: 1.0000 - val_loss: 1.6812 - val_accuracy: 0.8764\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 2.5539e-08 - accuracy: 1.0000 - val_loss: 1.7011 - val_accuracy: 0.8765\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 2.0061e-08 - accuracy: 1.0000 - val_loss: 1.7169 - val_accuracy: 0.8763\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 1.6375e-08 - accuracy: 1.0000 - val_loss: 1.7304 - val_accuracy: 0.8765\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 1.3697e-08 - accuracy: 1.0000 - val_loss: 1.7421 - val_accuracy: 0.8764\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 1.1700e-08 - accuracy: 1.0000 - val_loss: 1.7530 - val_accuracy: 0.8761\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 1.0196e-08 - accuracy: 1.0000 - val_loss: 1.7631 - val_accuracy: 0.8761\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 9.0197e-09 - accuracy: 1.0000 - val_loss: 1.7717 - val_accuracy: 0.8760\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 8.0872e-09 - accuracy: 1.0000 - val_loss: 1.7794 - val_accuracy: 0.8765\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 7.3341e-09 - accuracy: 1.0000 - val_loss: 1.7866 - val_accuracy: 0.8765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1R-XCz-4gEN"
      },
      "source": [
        "### Our accuracy and loss\n",
        "Let's plot out both the training and validation accuracy and loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6oPJSbM4gW8",
        "outputId": "103eb738-5596-4d1c-fc5d-2cc95b1954cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiqOLIsJ5bg8"
      },
      "source": [
        "What is your interpretation of the plots?\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "Okay, that definitely looks like overfitting. We will examine ways of ameliorating overfitting shortly.  \n",
        "\n",
        "Let's see how our network performs on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Byb8BGdL8Xtr",
        "outputId": "86aa04c1-ac3c-40f9-a396-f032134bd1bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "scoreSeg = network.evaluate(imdb_test_text, imdb_test_labels)\n",
        "print(\"Accuracy: \", scoreSeg[1])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 1.7855 - accuracy: 0.8758\n",
            "Accuracy:  0.8758000135421753\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaPCNeGgrTdF"
      },
      "source": [
        "Not bad for our first attempt at text classification!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3Y8Ck5hCfYk"
      },
      "source": [
        "## You try - Reuters Dataset\n",
        "\n",
        "The Reuters Dataset is another well-known machine learning dataset. The dataset consists of text from the Reuters newswire in 1987. We will use a subset of the data that contain texts from eight topics:\n",
        "\n",
        "\n",
        "\n",
        "Class | \t# train docs |\t# test docs\t| Total # docs\n",
        ":--- | --: | --: | --:\n",
        "acq |\t1596\t| 696\t| 2292\n",
        "crude | \t253 | \t121 |\t374\n",
        "earn\t| 2840\t| 1083\t| 3923\n",
        "grain\t|41 | \t10 | \t51\n",
        "interest | \t190\t| 81\t| 271\n",
        "money-fx\t| 206\t| 87\t| 293\n",
        "ship\t| 108\t| 36\t| 144\n",
        "trade\t| 251\t| 75\t| 326\n",
        "Total\t| 5485 |\t2189\t| 7674\n",
        "\n",
        "The training dataset is http://zacharski.org/files/courses/cs419/r8-train-all-terms.txt\n",
        "\n",
        "The test dataset is http://zacharski.org/files/courses/cs419/r8-test-all-terms.txt\n",
        "\n",
        "Can you build a network that will classify texts into one of 8 categories?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K7fSImcf1V3",
        "outputId": "0397e3e7-9137-48ed-f431-c8a3b7f8db1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget http://zacharski.org/files/courses/cs419/r8-train-all-terms.txt\n",
        "!wget http://zacharski.org/files/courses/cs419/r8-test-all-terms.txt"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-17 16:01:00--  http://zacharski.org/files/courses/cs419/r8-train-all-terms.txt\n",
            "Resolving zacharski.org (zacharski.org)... 198.199.65.227\n",
            "Connecting to zacharski.org (zacharski.org)|198.199.65.227|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3354435 (3.2M) [text/plain]\n",
            "Saving to: ‘r8-train-all-terms.txt’\n",
            "\n",
            "r8-train-all-terms. 100%[===================>]   3.20M  2.55MB/s    in 1.3s    \n",
            "\n",
            "2020-11-17 16:01:01 (2.55 MB/s) - ‘r8-train-all-terms.txt’ saved [3354435/3354435]\n",
            "\n",
            "--2020-11-17 16:01:01--  http://zacharski.org/files/courses/cs419/r8-test-all-terms.txt\n",
            "Resolving zacharski.org (zacharski.org)... 198.199.65.227\n",
            "Connecting to zacharski.org (zacharski.org)|198.199.65.227|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1195272 (1.1M) [text/plain]\n",
            "Saving to: ‘r8-test-all-terms.txt’\n",
            "\n",
            "r8-test-all-terms.t 100%[===================>]   1.14M  1.06MB/s    in 1.1s    \n",
            "\n",
            "2020-11-17 16:01:03 (1.06 MB/s) - ‘r8-test-all-terms.txt’ saved [1195272/1195272]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XDkqI2p8DOo",
        "outputId": "18816740-4ae6-4b40-dd55-dcc472f97081",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imdb.csv  imdb.zip  r8-test-all-terms.txt  r8-train-all-terms.txt  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaSDdBdw8IFQ",
        "outputId": "e094b82c-862f-495b-ca4a-b674ca5001ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "train_data = pd.read_csv(\"r8-train-all-terms.txt\", sep='\\t')\n",
        "test_data = pd.read_csv(\"r8-test-all-terms.txt\", sep='\\t')\n",
        "train_data"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>earn</td>\n",
              "      <td>champion products ch approves stock split cham...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>acq</td>\n",
              "      <td>computer terminal systems cpml completes sale ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>earn</td>\n",
              "      <td>cobanco inc cbco year net shr cts vs dlrs net ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>earn</td>\n",
              "      <td>am international inc am nd qtr jan oper shr lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>earn</td>\n",
              "      <td>brown forman inc bfd th qtr net shr one dlr vs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5480</th>\n",
              "      <td>earn</td>\n",
              "      <td>kelly oil and gas partners kly year dec shr ct...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5481</th>\n",
              "      <td>money-fx</td>\n",
              "      <td>japan seeks to strengthen paris currency accor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5482</th>\n",
              "      <td>earn</td>\n",
              "      <td>tcw convertible securities cvt sets dividend t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5483</th>\n",
              "      <td>money-fx</td>\n",
              "      <td>south korean won fixed at month high the bank ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5484</th>\n",
              "      <td>ship</td>\n",
              "      <td>australian unions launch new south wales strik...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5485 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         class                                               text\n",
              "0         earn  champion products ch approves stock split cham...\n",
              "1          acq  computer terminal systems cpml completes sale ...\n",
              "2         earn  cobanco inc cbco year net shr cts vs dlrs net ...\n",
              "3         earn  am international inc am nd qtr jan oper shr lo...\n",
              "4         earn  brown forman inc bfd th qtr net shr one dlr vs...\n",
              "...        ...                                                ...\n",
              "5480      earn  kelly oil and gas partners kly year dec shr ct...\n",
              "5481  money-fx  japan seeks to strengthen paris currency accor...\n",
              "5482      earn  tcw convertible securities cvt sets dividend t...\n",
              "5483  money-fx  south korean won fixed at month high the bank ...\n",
              "5484      ship  australian unions launch new south wales strik...\n",
              "\n",
              "[5485 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkudtjEi8H6j",
        "outputId": "c55af6f8-a782-45c8-96fc-b16cd0058859",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_text = train_data['text']\n",
        "train_label = train_data['class']\n",
        "test_text = test_data['text']\n",
        "test_label = test_data['class']\n",
        "train_data_text"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       champion products ch approves stock split cham...\n",
              "1       computer terminal systems cpml completes sale ...\n",
              "2       cobanco inc cbco year net shr cts vs dlrs net ...\n",
              "3       am international inc am nd qtr jan oper shr lo...\n",
              "4       brown forman inc bfd th qtr net shr one dlr vs...\n",
              "                              ...                        \n",
              "5480    kelly oil and gas partners kly year dec shr ct...\n",
              "5481    japan seeks to strengthen paris currency accor...\n",
              "5482    tcw convertible securities cvt sets dividend t...\n",
              "5483    south korean won fixed at month high the bank ...\n",
              "5484    australian unions launch new south wales strik...\n",
              "Name: text, Length: 5485, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvC3tJUiCppb",
        "outputId": "c66be2a7-3575-44ce-ad84-094a46d2e518",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(train_text)\n",
        "\n",
        "one_hot_train = tokenizer.texts_to_matrix(train_text, mode='tfidf')\n",
        "\n",
        "tokenizer.fit_on_texts(test_text)\n",
        "one_hot_test = tokenizer.texts_to_matrix(test_text, mode='tfidf')\n",
        "\n",
        "print(one_hot_train[0])\n",
        "print(one_hot_test[0])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         2.1239325  2.16894167 ... 0.         0.         0.        ]\n",
            "[0.         4.74633214 4.07241767 ... 0.         0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgR7g7nW8pPY",
        "outputId": "7d72d5bd-bb42-4333-d6e0-375294b30dc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "train_data_label = pd.get_dummies(train_label)\n",
        "test_data_label = pd.get_dummies(test_label)\n",
        "test_data_label"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acq</th>\n",
              "      <th>crude</th>\n",
              "      <th>earn</th>\n",
              "      <th>grain</th>\n",
              "      <th>interest</th>\n",
              "      <th>money-fx</th>\n",
              "      <th>ship</th>\n",
              "      <th>trade</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2184</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2185</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2186</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2187</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2188</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2189 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      acq  crude  earn  grain  interest  money-fx  ship  trade\n",
              "0       0      0     0      0         0         0     0      1\n",
              "1       0      0     0      1         0         0     0      0\n",
              "2       0      0     0      0         0         0     1      0\n",
              "3       1      0     0      0         0         0     0      0\n",
              "4       0      0     1      0         0         0     0      0\n",
              "...   ...    ...   ...    ...       ...       ...   ...    ...\n",
              "2184    0      0     0      0         0         1     0      0\n",
              "2185    0      0     0      0         0         0     0      1\n",
              "2186    0      1     0      0         0         0     0      0\n",
              "2187    1      0     0      0         0         0     0      0\n",
              "2188    0      0     0      0         0         0     1      0\n",
              "\n",
              "[2189 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFmveiK48pEw"
      },
      "source": [
        "network = models.Sequential()\n",
        "network.add(layers.Dense(512, activation='relu', input_shape=(5000,)))\n",
        "network.add(layers.Dense(256, activation='relu'))\n",
        "network.add(layers.Dense(128, activation='relu'))\n",
        "network.add(layers.Dense(8, activation='softmax'))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgbLl86L9V0Q",
        "outputId": "eb7d2626-818b-4b49-da7e-8c5c2e7de302",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "network.compile(optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "network.summary()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_20 (Dense)             (None, 512)               2560512   \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 8)                 1032      \n",
            "=================================================================\n",
            "Total params: 2,725,768\n",
            "Trainable params: 2,725,768\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_h1JbLy9g35",
        "outputId": "2b25e5e8-94a9-48d1-cca7-3533e7673edf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "history = network.fit(\n",
        "      one_hot_train, train_data_label,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=30,\n",
        "      validation_split=0.2,\n",
        "      validation_steps=50)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.9005 - accuracy: 0.7762 - val_loss: 0.5782 - val_accuracy: 0.8286\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.2299 - accuracy: 0.9505 - val_loss: 0.3344 - val_accuracy: 0.9034\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0911 - accuracy: 0.9850 - val_loss: 0.2632 - val_accuracy: 0.9289\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0404 - accuracy: 0.9943 - val_loss: 0.2101 - val_accuracy: 0.9462\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0196 - accuracy: 0.9977 - val_loss: 0.2383 - val_accuracy: 0.9380\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0112 - accuracy: 0.9979 - val_loss: 0.2404 - val_accuracy: 0.9471\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0079 - accuracy: 0.9982 - val_loss: 0.2351 - val_accuracy: 0.9499\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.2918 - val_accuracy: 0.9480\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.3437 - val_accuracy: 0.9444\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0041 - accuracy: 0.9989 - val_loss: 0.3301 - val_accuracy: 0.9471\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.3408 - val_accuracy: 0.9508\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0028 - accuracy: 0.9986 - val_loss: 0.3891 - val_accuracy: 0.9462\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0031 - accuracy: 0.9984 - val_loss: 0.3650 - val_accuracy: 0.9517\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0026 - accuracy: 0.9989 - val_loss: 0.3617 - val_accuracy: 0.9480\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0028 - accuracy: 0.9989 - val_loss: 0.3813 - val_accuracy: 0.9499\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.4451 - val_accuracy: 0.9462\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0027 - accuracy: 0.9989 - val_loss: 0.4240 - val_accuracy: 0.9499\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0025 - accuracy: 0.9989 - val_loss: 0.4500 - val_accuracy: 0.9490\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0023 - accuracy: 0.9989 - val_loss: 0.5230 - val_accuracy: 0.9426\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.4860 - val_accuracy: 0.9462\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0022 - accuracy: 0.9989 - val_loss: 0.5053 - val_accuracy: 0.9462\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0023 - accuracy: 0.9984 - val_loss: 0.5234 - val_accuracy: 0.9490\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.5108 - val_accuracy: 0.9499\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.5301 - val_accuracy: 0.9471\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0024 - accuracy: 0.9991 - val_loss: 0.5359 - val_accuracy: 0.9462\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0027 - accuracy: 0.9989 - val_loss: 0.5338 - val_accuracy: 0.9462\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0022 - accuracy: 0.9989 - val_loss: 0.5257 - val_accuracy: 0.9471\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0021 - accuracy: 0.9986 - val_loss: 0.6025 - val_accuracy: 0.9426\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.5706 - val_accuracy: 0.9435\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.5410 - val_accuracy: 0.9471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HZ95DyZ_Jyx"
      },
      "source": [
        ""
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri4W2UyP9gs7",
        "outputId": "cd925a7a-1a55-4f42-c939-a91cb3070ab4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "score = network.evaluate(one_hot_test, test_data_label)\n",
        "print(\"Accuracy: \", score[1])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69/69 [==============================] - 0s 3ms/step - loss: 9.3029 - accuracy: 0.5779\n",
            "Accuracy:  0.5778894424438477\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}